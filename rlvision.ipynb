{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlvision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKiCwxUq_QLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2B3mJkZd2-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -C drive/My\\ Drive/VOC2012/ -xvf drive/My\\ Drive/VOC2012/VOCtrainval_11-May-2012.tar "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqc8LvR_qUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages and load dataset\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import math\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import cv2\n",
        "\n",
        "VOC2012 = torchvision.datasets.VOCDetection(\"drive/My Drive/VOC2012\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ9KBiwJJs9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data input extraction\n",
        "\n",
        "print(VOC2012.__len__())\n",
        "\n",
        "img = VOC2012[0][0]\n",
        "h = int(VOC2012[0][1]['annotation']['size']['height'])\n",
        "w = int(VOC2012[0][1]['annotation']['size']['width'])\n",
        "\n",
        "print(w, h)\n",
        "\n",
        "bbox = VOC2012[0][1]['annotation']['object'][0]['bndbox']\n",
        "left = int(bbox['xmin'])\n",
        "upper = int(bbox['ymin'])\n",
        "right = int(bbox['xmax'])\n",
        "lower = int(bbox['ymax'])\n",
        "\n",
        "bbox_original = (0, 0, w, h)\n",
        "print(bbox_original)\n",
        "bbox = (left, upper, right, lower)\n",
        "print(bbox)\n",
        "\n",
        "display(img)\n",
        "img = img.crop(bbox)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2pMBOhH6IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define DQN with resnet preprocessing step\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # pre-trained convolutional network\n",
        "        conv = torchvision.models.resnet50(pretrained=True)\n",
        "        modules = list(conv.children())[:-1]\n",
        "        self.conv = nn.Sequential(*modules)\n",
        "        for p in conv.parameters():\n",
        "            p.requires_grad = False\n",
        "            \n",
        "        # deep Q-network\n",
        "        self.dqn = nn.Sequential(\n",
        "            nn.Linear(2138, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 9),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, img_t, action_history):\n",
        "        out = self.conv(img_t)\n",
        "        out = out.reshape(out.size(0), 2048)\n",
        "        out = torch.cat((out, action_history), dim=1)\n",
        "        out = self.dqn(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7wgH1mz6t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data loading and preprocessing functions\n",
        "\n",
        "State = namedtuple('State',\n",
        "                        ('image', 'bbox_observed', 'bbox_true', 'action_history'))\n",
        "\n",
        "def default_collate(batch):\n",
        "    states = []\n",
        "    for item in batch:\n",
        "        image = item[0]\n",
        "        action_history = torch.zeros(90)\n",
        "        h = int(item[1]['annotation']['size']['height'])\n",
        "        w = int(item[1]['annotation']['size']['width'])\n",
        "        bbox_observed = (0, 0, int(w/2), int(h/2))\n",
        "        obj = item[1]['annotation']['object']\n",
        "        if isinstance(obj, list):\n",
        "            bbox = obj[0]['bndbox']\n",
        "        else:\n",
        "            bbox = obj['bndbox']     \n",
        "        left = int(bbox['xmin'])\n",
        "        upper = int(bbox['ymin'])\n",
        "        right = int(bbox['xmax'])\n",
        "        lower = int(bbox['ymax'])\n",
        "        bbox_true = (left, upper, right, lower)\n",
        "        states.append(State(image, bbox_observed, bbox_true, action_history))\n",
        "    return states\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "def state_transform(states):\n",
        "    # return the transformed images and action_history for each state\n",
        "    img_observed = [state.image.crop(state.bbox_observed) for state in states]\n",
        "    img_t = torch.stack([transform(img) for img in img_observed]).to(device)\n",
        "    action_history = torch.stack([state.action_history for state in states]).to(device)\n",
        "    return img_t, action_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg4HyFw2LDGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define replay memory\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOLJAz2EnN-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reinforcement learning actions/state updates\n",
        "\n",
        "n_actions = 9\n",
        "\n",
        "def calculate_iou(state):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "\n",
        "    img_mask = np.zeros((image.height, image.width))\n",
        "    gt_mask = np.zeros((image.height, image.width))\n",
        "\n",
        "    x1, y1, x2, y2 = bbox_observed\n",
        "    img_mask[y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    x1, y1, x2, y2 = bbox_true\n",
        "    gt_mask[y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    img_and = cv2.bitwise_and(img_mask, gt_mask)\n",
        "    img_or = cv2.bitwise_or(img_mask, gt_mask)\n",
        "    j = np.count_nonzero(img_and)\n",
        "    i = np.count_nonzero(img_or)\n",
        "    iou = float(float(j)/float(i))\n",
        "    \n",
        "    return iou\n",
        "\n",
        "def update_action_history(action_history, action):\n",
        "    action_history_new = action_history.clone()\n",
        "    action_tmp = torch.zeros(9)\n",
        "    action_tmp[action] = 1\n",
        "    action = action_tmp\n",
        "      \n",
        "    last_actions = action_history_new[:81].clone()\n",
        "       \n",
        "    action_history_new[:9] = action\n",
        "    action_history_new[9:] = last_actions\n",
        "        \n",
        "    return action_history_new\n",
        " \n",
        "def take_action(state, action):\n",
        "    image, bbox_observed, bbox_true, action_history = state        \n",
        "    x1, y1, x2, y2 = bbox_observed\n",
        "    alph_w = int(0.2 * (x2 - x1))\n",
        "    alph_h = int(0.2 * (y2 - y1))\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    if action == 0: #horizontal move to the right\n",
        "        #if x2 + alph_w > image.width:\n",
        "        #    alph_w = image.width - x2\n",
        "        x1 += alph_w\n",
        "        x2 = min(x2 + alph_w, image.width)\n",
        "    elif action == 1: #horizontal move to the left\n",
        "        #if alph_w > x1:\n",
        "        #    alph_w = x1\n",
        "        x1 = max(x1 - alph_w, 0)\n",
        "        x2 -= alph_w\n",
        "    elif action == 2: #vertical move up\n",
        "        #if alph_h > y1:\n",
        "        #    alph_h = y1\n",
        "        y1 = max(y1 - alph_h, 0)\n",
        "        y2 -= alph_h\n",
        "    elif action == 3: #vertical move down\n",
        "        #if y2 + alph_h > image.height:\n",
        "        #    alph_h = image.height - y2\n",
        "        y1 += alph_h\n",
        "        y2 = min(y2 + alph_h, image.height)\n",
        "    elif action == 4: #scale up\n",
        "        #max_x_oob = max(alph_w - x1, x2 + alph_w - image.width)\n",
        "        #if max_x_oob > 0:\n",
        "        #    alph_w -= max_x_oob\n",
        "        x1 = max(x1 - math.floor(alph_w/2), 0)\n",
        "        x2 = min(x2 + math.floor(alph_w/2), image.width)\n",
        "        #max_y_oob = max(alph_h - y1, y2 + alph_h - image.height)\n",
        "        #if max_y_oob > 0:\n",
        "        #    alph_h -= max_y_oob\n",
        "        y1 = max(y1 - math.floor(alph_h/2), 0)\n",
        "        y2 = min(y2 + math.floor(alph_h/2), image.height)\n",
        "    elif action == 5: #scale down\n",
        "        x1 += math.floor(alph_w/2)\n",
        "        x2 -= math.floor(alph_w/2)\n",
        "        y1 += math.floor(alph_h/2)\n",
        "        y2 -= math.floor(alph_h/2)\n",
        "    elif action == 6: #decrease height (aspect ratio)\n",
        "        y1 += math.floor(alph_h/2)\n",
        "        y2 -= math.floor(alph_h/2)\n",
        "    elif action == 7: #decrease width (aspect ratio)\n",
        "        x1 += math.floor(alph_w/2)\n",
        "        x2 -= math.floor(alph_w/2)\n",
        "    elif action == 8: #trigger\n",
        "        done = True\n",
        "        \n",
        "    bbox_observed_new = (x1, y1, x2, y2)\n",
        "    action_history_new = update_action_history(action_history, action)\n",
        "    next_state = State(image, bbox_observed_new, bbox_true, action_history_new)\n",
        "    \n",
        "    iou_old = calculate_iou(state)\n",
        "    iou_new = calculate_iou(next_state)\n",
        "       \n",
        "    if done:\n",
        "        if iou_new >= 0.6:\n",
        "            reward = 3.0\n",
        "        else:\n",
        "            reward = -3.0\n",
        "    else:\n",
        "        reward = np.sign(iou_new - iou_old)\n",
        "        \n",
        "    return reward, next_state, done\n",
        "\n",
        "def find_positive_actions(state):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "    positive_actions = []\n",
        "    for i in range(n_actions):\n",
        "        reward, next_state, done = take_action(state, i)\n",
        "        if reward > 0:\n",
        "            positive_actions.append(i)\n",
        "    return positive_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCou_zXC-vuj",
        "colab_type": "code",
        "outputId": "35c2c390-98f9-43eb-ada6-7cbfc99ee847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Hyperparameters / utilities\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "policy_net = Net().to(device)\n",
        "target_net = Net().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "# training loop\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(VOC2012, batch_size=BATCH_SIZE, collate_fn=default_collate, shuffle=True)\n",
        "\n",
        "VOCtest = torchvision.datasets.VOCDetection(\"drive/My Drive/VOC2012\", image_set='val')\n",
        "test_loader = torch.utils.data.DataLoader(VOCtest, batch_size=1, collate_fn=default_collate, shuffle=True)\n",
        "test_iter = enumerate(test_loader)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "import timeit\n",
        "\n",
        "def select_action(img_t, action_history, states):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        # select best action from model with probability 1-epsilon\n",
        "        with torch.no_grad():\n",
        "            actions = policy_net(img_t, action_history)\n",
        "            return torch.max(actions, 1).indices\n",
        "    else:\n",
        "        # select random positive action with probability epsilon\n",
        "        actions = []\n",
        "        for state in states:\n",
        "            positive_actions = find_positive_actions(state)\n",
        "            if len(positive_actions) > 0:\n",
        "                action = random.choice(positive_actions)\n",
        "            else:\n",
        "                action = random.randrange(n_actions)\n",
        "            actions.append(action)\n",
        "        return torch.tensor(actions, device=device)\n",
        "\n",
        "total_time = 0\n",
        "print(\"First 10 DQN params (initialization):\", policy_net.state_dict()['dqn.0.weight'][0][:10])\n",
        "for i, states in enumerate(train_loader):\n",
        "    print(\"Running batch\", i)\n",
        "    batch_steps = 0\n",
        "    start = timeit.default_timer()\n",
        "    while len(states) > 0 and batch_steps < 100:\n",
        "        img_t, action_history = state_transform(states)\n",
        "        actions = select_action(img_t, action_history, states)\n",
        "        states_new = []\n",
        "        for j in range(actions.shape[0]):\n",
        "            action = actions[j].item()\n",
        "            state = states[j]\n",
        "            reward, next_state, done = take_action(state, action)\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            memory.push(state, action, next_state, reward)\n",
        "            if not done:\n",
        "                states_new.append(next_state)\n",
        "        optimize_model()\n",
        "        \n",
        "        if batch_steps % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            \n",
        "        states = states_new\n",
        "        batch_steps+=1\n",
        "    \n",
        "    # save visualization\n",
        "    _, s = test_iter.__next__()\n",
        "    localize(s[0], \"img_{}\".format(i))\n",
        "    \n",
        "    stop = timeit.default_timer()\n",
        "    t = (stop-start)/60\n",
        "    total_time += t\n",
        "    print(\"Finished batch {0} in {1:.2f} minutes.\".format(i, t))\n",
        "    print(\"Total time: {0:.2f} minutes.\".format(total_time))\n",
        "    print(\"First 10 DQN params after batch {0}:\".format(i), policy_net.state_dict()['dqn.0.weight'][0][:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 DQN params (initialization): tensor([ 3.5972e-03, -2.9684e-03, -8.1722e-03,  1.4963e-02,  1.0381e-02,\n",
            "         8.4182e-05,  1.4594e-02, -3.7783e-03,  1.9429e-02,  1.3995e-02],\n",
            "       device='cuda:0')\n",
            "Running batch 0\n",
            "Finished batch 0 in 2.24 minutes.\n",
            "Total time: 2.24 minutes.\n",
            "First 10 DQN params after batch 0: tensor([ 0.0045, -0.0029, -0.0073,  0.0141,  0.0117, -0.0002,  0.0147, -0.0041,\n",
            "         0.0198,  0.0127], device='cuda:0')\n",
            "Running batch 1\n",
            "Finished batch 1 in 3.62 minutes.\n",
            "Total time: 5.87 minutes.\n",
            "First 10 DQN params after batch 1: tensor([ 0.0072, -0.0033, -0.0067,  0.0155,  0.0135, -0.0010,  0.0162, -0.0024,\n",
            "         0.0198,  0.0146], device='cuda:0')\n",
            "Running batch 2\n",
            "Finished batch 2 in 3.58 minutes.\n",
            "Total time: 9.44 minutes.\n",
            "First 10 DQN params after batch 2: tensor([ 0.0076, -0.0016, -0.0036,  0.0162,  0.0143,  0.0002,  0.0186, -0.0029,\n",
            "         0.0201,  0.0150], device='cuda:0')\n",
            "Running batch 3\n",
            "Finished batch 3 in 4.22 minutes.\n",
            "Total time: 13.66 minutes.\n",
            "First 10 DQN params after batch 3: tensor([ 0.0071, -0.0015, -0.0026,  0.0153,  0.0145,  0.0002,  0.0177, -0.0036,\n",
            "         0.0201,  0.0150], device='cuda:0')\n",
            "Running batch 4\n",
            "Finished batch 4 in 4.29 minutes.\n",
            "Total time: 17.95 minutes.\n",
            "First 10 DQN params after batch 4: tensor([ 0.0076, -0.0009, -0.0017,  0.0141,  0.0135,  0.0009,  0.0172, -0.0032,\n",
            "         0.0208,  0.0146], device='cuda:0')\n",
            "Running batch 5\n",
            "Finished batch 5 in 4.17 minutes.\n",
            "Total time: 22.12 minutes.\n",
            "First 10 DQN params after batch 5: tensor([ 0.0063, -0.0022, -0.0012,  0.0121,  0.0124,  0.0009,  0.0179, -0.0029,\n",
            "         0.0207,  0.0138], device='cuda:0')\n",
            "Running batch 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T83o-UqGGpbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i, S = enumerate(train_loader).__next__()\n",
        "print(S[0])\n",
        "\n",
        "img_t, action_history = state_transform(S)\n",
        "actions = select_action(img_t, action_history, S)\n",
        "action = actions[0].item()\n",
        "print(actions, action)\n",
        "print(S[0])\n",
        "reward, next_state, done = take_action(S[0], action)\n",
        "print(\"reward:\", reward)\n",
        "print(\"next_state:\", next_state)\n",
        "print(\"done:\", done)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTOftgb9vwAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimization\n",
        "\n",
        "def get_last_action(state):\n",
        "    last_action = state.action_history[:9]\n",
        "    return last_action.nonzero().item()\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: get_last_action(s) != 8,\n",
        "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
        "    \n",
        "    non_final_next_states = [s for s in batch.next_state if get_last_action(s) != 8]\n",
        "    non_final_img_t, non_final_action_history = state_transform(non_final_next_states)\n",
        "    \n",
        "    state_batch = batch.state\n",
        "    action_batch = torch.tensor(batch.action, device=device)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "       \n",
        "    img_t, action_history = state_transform(state_batch)\n",
        "    \n",
        "    actions = policy_net(img_t, action_history)\n",
        "\n",
        "    state_action_values = policy_net(img_t, action_history).gather(1, action_batch.view(-1, 1))\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_img_t, non_final_action_history).max(1)[0].detach()\n",
        "\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        if param.grad is not None:\n",
        "            param.grad.data.clamp(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SjqBrWAnv_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualization\n",
        "\n",
        "from PIL import ImageDraw\n",
        "\n",
        "def draw_boxes(state):\n",
        "    image = state.image.copy()\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.rectangle(state.bbox_true, outline=(255,0,255))\n",
        "    draw.rectangle(state.bbox_observed, outline=(0,255,255))\n",
        "    return(image)\n",
        "\n",
        "def localize(state, name):\n",
        "    vis = draw_boxes(state)\n",
        "    w = state.image.width\n",
        "    h = state.image.height\n",
        "    for i in range(20):\n",
        "        img_t, action_history = state_transform([state])\n",
        "        action = policy_net(img_t, action_history).max(1).indices[0].item()\n",
        "        reward, state, done = take_action(state, action)\n",
        "        vis_new = Image.new('RGB', (vis.width + w, h))\n",
        "        vis_new.paste(vis)\n",
        "        vis_new.paste(draw_boxes(state), (vis.width, 0))\n",
        "        vis = vis_new\n",
        "        if done:\n",
        "            break\n",
        "    vis.save(\"drive/My Drive/visualization/{}.png\".format(name))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
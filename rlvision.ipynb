{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlvision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKiCwxUq_QLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2B3mJkZd2-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -C drive/My\\ Drive/VOC2012/ -xvf drive/My\\ Drive/VOC2012/VOCtrainval_11-May-2012.tar "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqc8LvR_qUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages and load dataset\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import math\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import cv2\n",
        "\n",
        "VOC2012 = torchvision.datasets.VOCDetection(\"drive/My Drive/VOC2012\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ9KBiwJJs9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data input extraction\n",
        "\n",
        "img = VOC2012[0][0]\n",
        "h = int(VOC2012[0][1]['annotation']['size']['height'])\n",
        "w = int(VOC2012[0][1]['annotation']['size']['width'])\n",
        "\n",
        "print(w, h)\n",
        "\n",
        "bbox = VOC2012[0][1]['annotation']['object'][0]['bndbox']\n",
        "left = int(bbox['xmin'])\n",
        "upper = int(bbox['ymin'])\n",
        "right = int(bbox['xmax'])\n",
        "lower = int(bbox['ymax'])\n",
        "\n",
        "bbox_original = (0, 0, w, h)\n",
        "print(bbox_original)\n",
        "bbox = (left, upper, right, lower)\n",
        "print(bbox)\n",
        "\n",
        "display(img)\n",
        "img = img.crop(bbox)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2pMBOhH6IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define DQN with resnet preprocessing step\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # pre-trained convolutional network\n",
        "        conv = torchvision.models.resnet50(pretrained=True)\n",
        "        modules = list(conv.children())[:-1]\n",
        "        self.conv = nn.Sequential(*modules)\n",
        "        for p in conv.parameters():\n",
        "            p.requires_grad = False\n",
        "            \n",
        "        # deep Q-network\n",
        "        self.dqn = nn.Sequential(\n",
        "            nn.Linear(2138, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 9),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, img_t, action_history):\n",
        "        x = self.conv(img_t)\n",
        "        x = x.reshape(x.size(0), 2048)\n",
        "        x = torch.cat((x, action_history), dim=1)\n",
        "        x = self.dqn(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7wgH1mz6t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data loading and preprocessing functions\n",
        "\n",
        "def default_collate(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    bboxes_observed = []\n",
        "    bboxes_true = []\n",
        "    action_history = [torch.zeros(90)] * len(images)\n",
        "    for item in batch:\n",
        "        h = int(item[1]['annotation']['size']['height'])\n",
        "        w = int(item[1]['annotation']['size']['width'])\n",
        "        bboxes_observed.append((0, 0, int(w/2), int(h/2)))\n",
        "        obj = item[1]['annotation']['object']\n",
        "        if isinstance(obj, list):\n",
        "            bbox = obj[0]['bndbox']\n",
        "        else:\n",
        "            bbox = obj['bndbox']     \n",
        "        left = int(bbox['xmin'])\n",
        "        upper = int(bbox['ymin'])\n",
        "        right = int(bbox['xmax'])\n",
        "        lower = int(bbox['ymax'])\n",
        "        bboxes_true.append((left, upper, right, lower))\n",
        "    return images, bboxes_observed, bboxes_true, action_history\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "    )])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg4HyFw2LDGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define replay memory\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qab0ORIOqIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: action selection / training\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "n_actions = 9\n",
        "\n",
        "policy_net = Net().to(device)\n",
        "target_net = Net().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(img_t, action_history):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(img_t, action_history).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "    \n",
        "#for (i, [images, bboxes_observed, bboxes_true, action_history]) in enumerate(train_loader):\n",
        "#    img_observed = [img.crop(bbox) for (img, bbox) in zip(images, bboxes_observed)]\n",
        "#    img_t = torch.stack([transform(img) for img in img_observed]).to(device)\n",
        "#    action_history = action_history.to(device)\n",
        "#    print(net(img_t, action_history))\n",
        "#    if i == 0:\n",
        "#        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOLJAz2EnN-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_iou(state):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "\n",
        "    img_mask = np.zeros((image.height, image.width))\n",
        "    gt_mask = np.zeros((image.height, image.width))\n",
        "\n",
        "    x1, y1, x2, y2 = bbox_observed\n",
        "    img_mask[y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    x1, y1, x2, y2 = bbox_true\n",
        "    gt_mask[y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    img_and = cv2.bitwise_and(img_mask, gt_mask)\n",
        "    img_or = cv2.bitwise_or(img_mask, gt_mask)\n",
        "    j = np.count_nonzero(img_and)\n",
        "    i = np.count_nonzero(img_or)\n",
        "    iou = float(float(j)/float(i))\n",
        "    \n",
        "    return iou\n",
        "\n",
        "def update_action_history(action_history, action):\n",
        "    action_tmp = torch.zeros(9)\n",
        "    action_tmp[action] = 1\n",
        "    action = action_tmp\n",
        "      \n",
        "    last_actions = action_history[:81].clone()\n",
        "       \n",
        "    action_history[:9] = action\n",
        "    action_history[9:] = last_actions\n",
        "        \n",
        "    return action_history\n",
        " \n",
        "def take_action(state, action):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "        \n",
        "    x1, y1, x2, y2 = bbox_observed\n",
        "    \n",
        "    alph_w = int(0.2 * (x2 - x1))\n",
        "    alph_h = int(0.2 * (y2 - y1))\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    if action == 0: #horizontal move to the right\n",
        "        #if x2 + alph_w > image.width:\n",
        "        #    alph_w = image.width - x2\n",
        "        x1 += alph_w\n",
        "        x2 = min(x2 + alph_w, image.width)\n",
        "    elif action == 1: #horizontal move to the left\n",
        "        #if alph_w > x1:\n",
        "        #    alph_w = x1\n",
        "        x1 = max(x1 - alph_w, 0)\n",
        "        x2 -= alph_w\n",
        "    elif action == 2: #vertical move up\n",
        "        #if alph_h > y1:\n",
        "        #    alph_h = y1\n",
        "        y1 = max(y1 - alph_h, 0)\n",
        "        y2 -= alph_h\n",
        "    elif action == 3: #vertical move down\n",
        "        #if y2 + alph_h > image.height:\n",
        "        #    alph_h = image.height - y2\n",
        "        y1 += alph_h\n",
        "        y2 = min(y2 + alph_h, image.height)\n",
        "    elif action == 4: #scale up\n",
        "        #max_x_oob = max(alph_w - x1, x2 + alph_w - image.width)\n",
        "        #if max_x_oob > 0:\n",
        "        #    alph_w -= max_x_oob\n",
        "        x1 = max(x1 - math.floor(alph_w/2), 0)\n",
        "        x2 = min(x2 + math.floor(alph_w/2), image.width)\n",
        "        #max_y_oob = max(alph_h - y1, y2 + alph_h - image.height)\n",
        "        #if max_y_oob > 0:\n",
        "        #    alph_h -= max_y_oob\n",
        "        y1 = max(y1 - math.floor(alph_h/2), 0)\n",
        "        y2 = min(y2 + math.floor(alph_h/2), image.height)\n",
        "    elif action == 5: #scale down\n",
        "        x1 += math.floor(alph_w/2)\n",
        "        x2 -= math.floor(alph_w/2)\n",
        "        y1 += math.floor(alph_h/2)\n",
        "        y2 -= math.floor(alph_h/2)\n",
        "    elif action == 6: #decrease height (aspect ratio)\n",
        "        y1 += math.floor(alph_h/2)\n",
        "        y2 -= math.floor(alph_h/2)\n",
        "    elif action == 7: #decrease width (aspect ratio)\n",
        "        x1 += math.floor(alph_w/2)\n",
        "        x2 -= math.floor(alph_w/2)\n",
        "    elif action == 8: #trigger\n",
        "        done = True\n",
        "        \n",
        "    bbox_observed_new = (x1, y1, x2, y2)\n",
        "    action_history_new = update_action_history(action_history, action)\n",
        "    state_new = (image, bbox_observed_new, bbox_true, action_history_new)\n",
        "        \n",
        "    #print(\"Action taken:\", action)\n",
        "    #print(\"Old BBOX:\", bbox_observed)\n",
        "    #display(image.crop(bbox_observed))\n",
        "    #print(\"New BBOX:\", bbox_observed_new)\n",
        "    #display(image.crop(bbox_observed_new))\n",
        "    \n",
        "    iou_old = calculate_iou(state)\n",
        "    iou_new = calculate_iou(state_new)\n",
        "    \n",
        "    #print(\"Old IOU:\", iou_old)\n",
        "    #print(\"New IOU:\", iou_new)\n",
        "    \n",
        "    if done:\n",
        "        if iou_new >= 0.6:\n",
        "            reward = 3.0\n",
        "        else:\n",
        "            reward = -3.0\n",
        "    else:\n",
        "        reward = np.sign(iou_new - iou_old)\n",
        "        \n",
        "    print(\"Reward Received:\", reward)\n",
        "    return reward, state_new, done\n",
        "\n",
        "def find_positive_actions(state):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "    positive_actions = []\n",
        "    for i in range(n_actions):\n",
        "        print(i)\n",
        "        reward, state_new, done = take_action(state, i)\n",
        "        if reward > 0:\n",
        "            positive_actions.append(i)\n",
        "    print(\"TEST:\", positive_actions)\n",
        "    return positive_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCou_zXC-vuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(VOC2012, batch_size=1, collate_fn=default_collate, shuffle=True, num_workers=4)\n",
        "\n",
        "(i, states) = enumerate(train_loader).__next__()\n",
        "\n",
        "#images, bboxes_observed, bboxes_true, action_history = states\n",
        "\n",
        "#img_observed = [img.crop(bbox) for (img, bbox) in zip(images, bboxes_observed)]\n",
        "#img_t = torch.stack([transform(img) for img in img_observed]).to(device)\n",
        "#action_history = torch.stack(action_history).to(device)\n",
        "\n",
        "#action = select_action(img_t, action_history).item()\n",
        "\n",
        "state = zip(*states).__next__()\n",
        "state_orig = state\n",
        "\n",
        "for i in range(20):\n",
        "    image, bbox_observed, bbox_true, action_history = state\n",
        "    if i == 0:\n",
        "        display(image)\n",
        "        display(image.crop(bbox_true))\n",
        "    display(image.crop(bbox_observed))\n",
        "    positive_actions = find_positive_actions(state)\n",
        "    if len(positive_actions) > 0:\n",
        "        action = random.choice(positive_actions)\n",
        "    else:\n",
        "        action = random.randrange(n_actions)\n",
        "    print(\"action selected:\", action)\n",
        "    \n",
        "    reward, state, done = take_action(state, action)\n",
        "    if reward < 0:\n",
        "        state_list.append(state_old)\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}